# -*- coding: utf-8 -*-
"""run_create_pretraining_512_base

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sGff_fA06dVMZ-51sFH3FruhAgkIL0UA
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import re
import os

def load_data(file_path):
  global vocabs
  texts = []
  nseq = 0
  data = re.split(r'(^>.*)', ''.join(open(file_path).readlines()) , flags=re.M)
  for i in range(2, len(data), 2):
    nseq = nseq + 1
    text = list(data[i].replace('\n','').replace('\x1a',''))
    texts.append(' '.join(text))
  print(f"Number of sequences: {nseq}")
  print(f"Number of samples: {len(texts)}")
  return texts

ds = load_data('/content/drive/My Drive/BERT/uniprot.fasta')
with open('fasta.txt','w') as fout:
  fout.write('\n'.join(ds))

!git clone https://github.com/google-research/bert

from google.colab import auth
auth.authenticate_user()

!python ./bert/create_pretraining_data.py \
  --input_file=./fasta.txt \
  --output_file='gs://bert_uniprot/tf.tfrecord' \
  --vocab_file='gs://bert_uniprot/vocabs.txt' \
  --do_lower_case=False \
  --max_seq_length=512 \
  --max_predictions_per_seq=20 \
  --do_whole_word_mask=True \
  --random_seed=312587 \
  --dupe_factor=5

import datetime
import json
import os
import pprint
import random
import string
import sys
import tensorflow as tf

assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'
TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']
print('TPU address is', TPU_ADDRESS)

from google.colab import auth
auth.authenticate_user()
with tf.Session(TPU_ADDRESS) as session:
  print('TPU devices:')
  pprint.pprint(session.list_devices())

  # Upload credentials to TPU.
  with open('/content/adc.json', 'r') as f:
    auth_info = json.load(f)
  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
  # Now credentials are set for all future sessions on this TPU.

!python ./bert/run_pretraining.py \
--input_file='gs://bert_uniprot/tf.tfrecord' \
--output_dir='gs://bert_uniprot/bert_base_L-12_H-768_A-12' \
--do_train=True \
--do_eval=True \
--bert_config_file='gs://bert_uniprot/bert_base_L-12_H-768_A-12.json' \
--train_batch_size=16 \
--max_seq_length=512 \
--learning_rate=2e-5 \
--num_train_steps=1000000 \
--use_tpu=True \
--tpu_name='grpc://10.52.85.58:8470'